{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a214f62",
   "metadata": {},
   "source": [
    "# Function definition for BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17a47c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unittest\n",
    "from math import exp, log, fsum\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = sorted(str)\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def mod_ngram_precision_count(cand_sent, ref_list, sum_count=True):\n",
    "  cand_count = word_count(cand_sent)\n",
    "  max_ref_count = dict.fromkeys(cand_count, 0)\n",
    "  ref_counts = [word_count(ref) for ref in ref_list]\n",
    "\n",
    "  for ref_count in ref_counts:\n",
    "    for key, value in ref_count.items():\n",
    "      if key in max_ref_count:\n",
    "          if max_ref_count[key] < value: # update max count in refs\n",
    "            max_ref_count[key] = value\n",
    "  for key, value in max_ref_count.items():\n",
    "    if cand_count[key] > value: # if candidate count more than max ref count\n",
    "      cand_count[key] = value # clip with max ref count\n",
    "  if sum_count==True:    \n",
    "    return sum(cand_count.values())\n",
    "  else:\n",
    "    return cand_count\n",
    "\n",
    "\n",
    "def ngram(text, gram_count):\n",
    "  return [tuple(text[i:i+gram_count]) for i in range(len(text)-gram_count+1)]\n",
    "\n",
    "\n",
    "# sentence bleu\n",
    "def bleu_by_sent(cand_sent, ref_list):\n",
    "  closest_ref_len = min([len(i) for i in ref_list], key=lambda x:abs(x-len(cand_sent)))\n",
    "  bp = min(1, exp(1 - closest_ref_len / len(cand_sent)))\n",
    "  prec = [mod_ngram_precision_count(ngram(cand_sent, i), [ngram(ref, i) for ref in ref_list]) / len(ngram(cand_sent, i)) for i in range(1,5)]\n",
    "  ls_prec = [0.25 * log(prec_n) if prec_n != 0 else 0.25 * log(sys.float_info.min) for prec_n in prec]\n",
    "  return bp * exp(fsum(ls_prec))\n",
    "\n",
    "\n",
    "# corpus bleu\n",
    "def bleu_by_corpus(ls_cand, ls_ref):\n",
    "    \n",
    "    len_can = []\n",
    "    len_ref = []\n",
    "    prec_crr = []\n",
    "    prec_total = []\n",
    "\n",
    "    for i in range(len(ls_cand)):\n",
    "        cand_sent = ls_cand[i]\n",
    "        ref_list = ls_ref[i]\n",
    "\n",
    "        len_can.append(len(cand_sent))\n",
    "        len_ref.append(min([len(i) for i in ref_list], key=lambda x:abs(x-len(cand_sent))))\n",
    "        \n",
    "        prec_crr.append([mod_ngram_precision_count(ngram(cand_sent, i), [ngram(ref, i) for ref in ref_list]) for i in range(1,5)])\n",
    "        prec_total.append([len(ngram(cand_sent, i)) for i in range(1,5)])\n",
    "        \n",
    "    #precision cnt\n",
    "    arr_crr = np.array(prec_crr)\n",
    "    arr_total = np.array(prec_total)\n",
    "    \n",
    "    crr = [np.sum(arr_crr[:,i]) for i in range(len(arr_crr[0]))]\n",
    "    total = [np.sum(arr_total[:,i]) for i in range(len(arr_total[0]))]\n",
    "    p_n = [crr[i]/total[i] for i in range(len(crr))]\n",
    "\n",
    "    bp = min(1, exp(1 -  fsum(len_ref) / fsum(len_can)))\n",
    "    precision = [0.25 * log(prec_n) if prec_n != 0 else 0.25 * log(sys.float_info.min) for prec_n in p_n]\n",
    "    \n",
    "    return (bp * exp(fsum(precision)), p_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa634b",
   "metadata": {},
   "source": [
    "# Test of BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f0dfa",
   "metadata": {},
   "source": [
    "#### sentence BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86759bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk's : 0.4671379777282001\n",
      "scratch: 0.4671379777282001\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "cand = \"the cat the cat on the mat\"\n",
    "#cand = \"the the the the the the the the\"\n",
    "ref_1 = \"the cat is on the mat\"\n",
    "ref_2 = \"there is a cat on the mat\"\n",
    "\n",
    "cand = cand.split()\n",
    "ref_1 = ref_1.split()\n",
    "ref_2 = ref_2.split()\n",
    "\n",
    "print(\"nltk's :\", bleu_by_sent(cand, [ref_1, ref_2]))\n",
    "print('scratch:', sentence_bleu(references=[ref_1, ref_2], hypothesis=cand, weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3148e7",
   "metadata": {},
   "source": [
    "#### corpus BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02f2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk's :  0.7145484803707527\n",
      "scratch:  (0.7145484803707527, [0.8597285067873304, 0.7425968109339408, 0.6697247706422018, 0.6096997690531177])\n"
     ]
    }
   ],
   "source": [
    "cand = ['Small business owners have wasted no time in applying for COVID-19 compensation on the first day of the money becoming available to them.',\n",
    "       'Minister Lee said as the test run of the application system was conducted without overload errors, it was kept open.',\n",
    "       'Out of the 62 trillion won worth supplementary budget that passed the National Assembly on Sunday, 23 trillion won will be doled out to small businesses hard hit by the protracted pandemic.']\n",
    "\n",
    "ref = [[\"Small business owners didn't waste time applying for COVID-19 compensation on the first day the funds were available.\"],\n",
    "      ['Minister Lee said the application system remained open because the commissioning went without overload errors.'],\n",
    "      ['Of the 62 trillion won in the supplementary budget passed by the National Assembly on the 1st, 23 trillion won will be invested in small businesses that have been hit directly by the prolonged Corona 19.']]\n",
    "\n",
    "\n",
    "print(\"nltk's : \", corpus_bleu(ref, cand, weights=(0.25,0.25,0.25,0.25)))\n",
    "print('scratch: ', bleu_by_corpus(cand, ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622d82b",
   "metadata": {},
   "source": [
    "# BLEU of FR-EN and ZH-KO models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f96b14",
   "metadata": {},
   "source": [
    "#### Preparing reference and output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "492255f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/labdata/ko_zh_test_output.txt', 'r', encoding='utf-8') as f:\n",
    "    zh_ko = f.read()\n",
    "    \n",
    "with open('../data/labdata/fr_en_test_output.txt', 'r', encoding='utf-8') as f:\n",
    "    fr_en = f.read()\n",
    "    \n",
    "with open('../data/labdata/fr_test.txt', 'r', encoding='utf-8') as f:\n",
    "    fr = f.read()\n",
    "    \n",
    "with open('../data/labdata/ko_test.txt', 'r', encoding='utf-8') as f:\n",
    "    ko = f.read()\n",
    "    \n",
    "with open('../data/labdata/en_test.txt', 'r', encoding='utf-8') as f:\n",
    "    en = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b1c76da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Chinese sents\n",
      "predicted:  20000\n",
      "reference:  20000\n",
      "\n",
      "\n",
      "length of English sents\n",
      "predicted:  20000\n",
      "reference:  20000\n"
     ]
    }
   ],
   "source": [
    "pred_zh_ko = zh_ko.split('\\n')\n",
    "pred_fr_en = fr_en.split('\\n')\n",
    "ref_en = en.split('\\n')\n",
    "ref_ko = ko.split('\\n')\n",
    "ref_fr = fr.split('\\n')\n",
    "\n",
    "print('length of Chinese sents')\n",
    "print('predicted: ', len(pred_zh_ko))\n",
    "print('reference: ', len(ref_ko))\n",
    "print('\\n')\n",
    "\n",
    "print('length of English sents')\n",
    "print('predicted: ', len(pred_fr_en))\n",
    "print('reference: ', len(ref_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf26e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_en = []\n",
    "pred_en = []\n",
    "\n",
    "for i in range(len(ref_ko)):\n",
    "    corr = ref_en[i].replace('<UNK>', '').replace('   ', ' ')\n",
    "    pred = pred_fr_en[i].replace('<UNK>', '').replace('   ', ' ')\n",
    "    \n",
    "    corr_en.append([corr])\n",
    "    pred_en.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4cd373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_ko = []\n",
    "pred_ko = []\n",
    "\n",
    "for i in range(len(ref_ko)):\n",
    "    corr = ref_ko[i].replace('<UNK>', '').replace('   ', ' ')\n",
    "    pred = pred_zh_ko[i].replace('<UNK>', '').replace('   ', ' ')\n",
    "    \n",
    "    corr_ko.append([corr])\n",
    "    pred_ko.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181e2bf",
   "metadata": {},
   "source": [
    "#### Results of BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e487d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French-English result\n",
      "nltk's :  0.47989226890972836\n",
      "scratch:  (0.4798990451982229, [0.7517812218147842, 0.5567918179838579, 0.4020321140592466, 0.31517697251653387])\n"
     ]
    }
   ],
   "source": [
    "print('French-English result')\n",
    "print(\"nltk's : \", corpus_bleu(corr_en, pred_en, weights=(0.25,0.25,0.25,0.25)))\n",
    "print('scratch: ', bleu_by_corpus(pred_en, corr_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b42d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese-Korean result\n",
      "nltk's :  0.25218944149054745\n",
      "scratch:  (0.25218944149054745, [0.7129282397266276, 0.4927147061446535, 0.3200227004449287, 0.19989457134930178])\n"
     ]
    }
   ],
   "source": [
    "print('Chinese-Korean result')\n",
    "print(\"nltk's : \", corpus_bleu(corr_ko, pred_ko, weights=(0.25,0.25,0.25,0.25)))\n",
    "print('scratch: ', bleu_by_corpus(pred_ko, corr_ko))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
