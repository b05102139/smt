{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410a1b73",
   "metadata": {},
   "source": [
    "# import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae59cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/IMS-machine-translation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855b3708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42m'[원천]ko2zh_social_science_training_csv'\u001b[0m/     \u001b[01;32mko2zh_kpop_1_training.csv\u001b[0m*\r\n",
      "\u001b[34;42m'[원천]ko2zh_social_science_validation_csv'\u001b[0m/   \u001b[01;32mko2zh_kpop_2_validation.csv\u001b[0m*\r\n",
      "\u001b[34;42m'[원천]ko2zh_tech_training_csv'\u001b[0m/               \u001b[01;32mko2zh_law_1_training.csv\u001b[0m*\r\n",
      "\u001b[34;42m'[원천]ko2zh_tech_validation_csv'\u001b[0m/             \u001b[01;32mko2zh_law_2_validation.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_auto_1_training.csv\u001b[0m*                    \u001b[01;32mko2zh_medical_1_training.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_auto_2_validation.csv\u001b[0m*                  \u001b[01;32mko2zh_medical_2_validation.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_edu_1_training.csv\u001b[0m*                     \u001b[01;32mko2zh_patent_1_training.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_edu_2_validation.csv\u001b[0m*                   \u001b[01;32mko2zh_patent_2_validation.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_finance_1_training.csv\u001b[0m*                 \u001b[01;32mko2zh_politics_1_training.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_finance_2_validation.csv\u001b[0m*               \u001b[01;32mko2zh_politics_2_validation.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_folk_1_training.csv\u001b[0m*                    \u001b[01;32mko2zh_society_1_training.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_folk_2_validation.csv\u001b[0m*                  \u001b[01;32mko2zh_society_2_validation.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_it_tech_1_training.csv\u001b[0m*                 \u001b[01;32mkorean-chinese.csv\u001b[0m*\r\n",
      " \u001b[01;32mko2zh_it_tech_2_validation.csv\u001b[0m*               \u001b[01;32mkorean-chinese.zip\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/ko-zh\\ 2.3m\\ parallel/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f9caa",
   "metadata": {},
   "source": [
    "read csv using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "717b04c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 9)\n",
      "(20000, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../data/ko-zh 2.3m parallel/ko2zh_society_1_training.csv')\n",
    "df2 = pd.read_csv('../data/ko-zh 2.3m parallel/ko2zh_society_2_validation.csv')\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180aca96",
   "metadata": {},
   "source": [
    "dataframe shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "495c5c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 9)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0741",
   "metadata": {},
   "source": [
    "select only Korean and Chinese text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d14e8a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kr</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다.</td>\n",
       "      <td>这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>헌재는 \"정당가입 금지는 공무원의 정치적 중립성을 보장하고, 교육의 중립성을 확보하...</td>\n",
       "      <td>宪法裁判所解释道：\"禁止加入政党是为了保障公务员的政治中立性,确保教育的中立性,目的的正当性...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...</td>\n",
       "      <td>大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...</td>\n",
       "      <td>在首尔某公寓里,24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다.</td>\n",
       "      <td>从工作的立场上看,可以说是优点和缺点混杂在一起的形态。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  kr  \\\n",
       "0    루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다.   \n",
       "1  헌재는 \"정당가입 금지는 공무원의 정치적 중립성을 보장하고, 교육의 중립성을 확보하...   \n",
       "2  대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...   \n",
       "3  서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...   \n",
       "4               일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다.   \n",
       "\n",
       "                                                  zh  \n",
       "0                   这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策。  \n",
       "1  宪法裁判所解释道：\"禁止加入政党是为了保障公务员的政治中立性,确保教育的中立性,目的的正当性...  \n",
       "2  大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...  \n",
       "3         在首尔某公寓里,24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰。  \n",
       "4                        从工作的立场上看,可以说是优点和缺点混杂在一起的形态。  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['한국어', '중국어']].rename(columns={\"한국어\": \"kr\", \"중국어\": \"zh\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91cfefb",
   "metadata": {},
   "source": [
    "reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30b52c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kr</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다.</td>\n",
       "      <td>这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>헌재는 \"정당가입 금지는 공무원의 정치적 중립성을 보장하고, 교육의 중립성을 확보하...</td>\n",
       "      <td>宪法裁判所解释道：\"禁止加入政党是为了保障公务员的政治中立性,确保教育的中立性,目的的正当性...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...</td>\n",
       "      <td>大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...</td>\n",
       "      <td>在首尔某公寓里,24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다.</td>\n",
       "      <td>从工作的立场上看,可以说是优点和缺点混杂在一起的形态。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  kr  \\\n",
       "0    루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다.   \n",
       "1  헌재는 \"정당가입 금지는 공무원의 정치적 중립성을 보장하고, 교육의 중립성을 확보하...   \n",
       "2  대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...   \n",
       "3  서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...   \n",
       "4               일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다.   \n",
       "\n",
       "                                                  zh  \n",
       "0                   这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策。  \n",
       "1  宪法裁判所解释道：\"禁止加入政党是为了保障公务员的政治中立性,确保教育的中立性,目的的正当性...  \n",
       "2  大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...  \n",
       "3         在首尔某公寓里,24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰。  \n",
       "4                        从工作的立场上看,可以说是优点和缺点混杂在一起的形态。  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69758eb",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a66fb",
   "metadata": {},
   "source": [
    "define functions for preprocessing to include only alphabets, numbers, Korean and Chinese letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ec6cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ko_preprocess(s):\n",
    "    r = re.findall('[a-zA-Z0-9가-힣 ]', s)\n",
    "    r = ''.join(r).strip().replace('   ', ' ').replace('  ', ' ')\n",
    "    return r\n",
    "\n",
    "def zh_preprocess(s):\n",
    "    r = re.findall('[a-zA-Z0-9\\u4e00-\\u9fa5 ]', s)\n",
    "    r = ''.join(r).strip().replace('   ', ' ').replace('  ', ' ')\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117fe3f",
   "metadata": {},
   "source": [
    "test the preprocessing function for korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05fcf365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다.\n",
      "루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0][0])\n",
    "print(ko_preprocess(df.loc[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3c735",
   "metadata": {},
   "source": [
    "test the preprocessing function for chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4418d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策。\n",
      "这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0][1])\n",
    "print(zh_preprocess(df.loc[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac60af5",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f5d5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 180000/180000 [00:26<00:00, 6774.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    for j in range(2):\n",
    "        if j == 0:\n",
    "            r = ko_preprocess(df.loc[i][j])\n",
    "            df.loc[i][j] = r\n",
    "        else:\n",
    "            r = zh_preprocess(df.loc[i][j])\n",
    "            df.loc[i][j] = r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bde3d",
   "metadata": {},
   "source": [
    "the result of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4988699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kr</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다</td>\n",
       "      <td>这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>헌재는 정당가입 금지는 공무원의 정치적 중립성을 보장하고 교육의 중립성을 확보하기 ...</td>\n",
       "      <td>宪法裁判所解释道禁止加入政党是为了保障公务员的政治中立性确保教育的中立性目的的正当性和手段的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...</td>\n",
       "      <td>大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...</td>\n",
       "      <td>在首尔某公寓里24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다</td>\n",
       "      <td>从工作的立场上看可以说是优点和缺点混杂在一起的形态</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  kr  \\\n",
       "0     루즈벨트 행정부가 대공황으로 무너진 미국사회를 재건하기 위해 실행한 제반 정책이었다   \n",
       "1  헌재는 정당가입 금지는 공무원의 정치적 중립성을 보장하고 교육의 중립성을 확보하기 ...   \n",
       "2  대전지법 민사21부는 신성철 카이스트 총장이 김성진 아이카이이스트 대표를 상대로 제...   \n",
       "3  서울의 한 아파트에서 24시간씩 맞교대 경비업무를 하는 A씨는 한 입주민의 밤낮없는...   \n",
       "4                일하는 입장에선 장점과 단점이 혼재해 있는 형태라고 볼 수 있다   \n",
       "\n",
       "                                                  zh  \n",
       "0                    这是罗斯福政府为了重建因大萧条而崩溃的美国社会而实施的各项政策  \n",
       "1  宪法裁判所解释道禁止加入政党是为了保障公务员的政治中立性确保教育的中立性目的的正当性和手段的...  \n",
       "2  大田地方法院民事21部在申成哲KAIST总长以金成镇IKAIST代表为对象提出的禁止相互使用...  \n",
       "3           在首尔某公寓里24小时进行轮班警备工作的A某被一个居民不分昼夜的民怨和威胁所困扰  \n",
       "4                          从工作的立场上看可以说是优点和缺点混杂在一起的形态  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b488dc",
   "metadata": {},
   "source": [
    "# save to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2118cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/IMS-machine-translation'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79117de1",
   "metadata": {},
   "source": [
    "save each preprocessed column as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b5fd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt('../data/labdata/ko.txt', df['kr'].values, fmt='%s')\n",
    "np.savetxt('../data/labdata/zh.txt', df['zh'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf10738",
   "metadata": {},
   "source": [
    "# read text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1557a9",
   "metadata": {},
   "source": [
    "read text file and split each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "813ed1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/labdata/ko.txt') as f:\n",
    "    ko_txt = f.read()\n",
    "    \n",
    "with open('../data/labdata/zh.txt') as f:\n",
    "    zh_txt = f.read()\n",
    "    \n",
    "ko = ko_txt.split('\\n')\n",
    "zh = zh_txt.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a66b7",
   "metadata": {},
   "source": [
    "length of each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab10b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180001\n",
      "180001\n"
     ]
    }
   ],
   "source": [
    "print(len(ko))\n",
    "print(len(zh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7948c4",
   "metadata": {},
   "source": [
    "exclude the last empty list element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f3c7124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "180000\n"
     ]
    }
   ],
   "source": [
    "ko = ko[:-1]\n",
    "zh = zh[:-1]\n",
    "\n",
    "print(len(ko))\n",
    "print(len(zh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5311f9",
   "metadata": {},
   "source": [
    "# make vocab list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bed93",
   "metadata": {},
   "source": [
    "make unigram vocabulary for Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93dc285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [19:27, 154.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from operator import or_\n",
    "from functools import reduce\n",
    "\n",
    "# korean\n",
    "ko_vocab = set()\n",
    "\n",
    "for i, string in tqdm(enumerate(ko)):\n",
    "    ls_string = set(string.split())\n",
    "    ko_vocab = reduce(or_, [ko_vocab, ls_string])\n",
    "    \n",
    "ko_vocab = list(ko_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9381d",
   "metadata": {},
   "source": [
    "make unigram vocabulary for Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "011465dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [08:39, 346.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# chinese\n",
    "zh_vocab = set()\n",
    "\n",
    "for i, string in tqdm(enumerate(zh)):\n",
    "    ls_string = set(string.split())\n",
    "    zh_vocab = reduce(or_, [zh_vocab, ls_string])\n",
    "    \n",
    "zh_vocab = list(zh_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51952b",
   "metadata": {},
   "source": [
    "the number of unigram vocab for Korean and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a86d97c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336782\n",
      "183119\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_vocab))\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad153b",
   "metadata": {},
   "source": [
    "# make vocab count dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097faf6",
   "metadata": {},
   "source": [
    "define a function which takes an input sentence and returns it as unigram tokens with the end of sentence symbol \\\"\\</w>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7038cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate each character\n",
    "def sent_preprocessing(sent):\n",
    "    words = list(sent)\n",
    "    words.append('</w>')\n",
    "    \n",
    "    string = ''\n",
    "    for w in words:\n",
    "        string += w + ' '\n",
    "    string = string[:-1]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbedde5",
   "metadata": {},
   "source": [
    "make a vocab count dictionary for Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd40b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_vocab_dict = {}\n",
    "\n",
    "for v in ko_vocab:\n",
    "    key = sent_preprocessing(v)\n",
    "    cnt = ko_txt.count(v)\n",
    "    ko_vocab_dict[key] = cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001890fc",
   "metadata": {},
   "source": [
    "make a vocab count dictionary for Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa9c59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab_dict = {}\n",
    "\n",
    "for v in zh_vocab:\n",
    "    key = sent_preprocessing(v)\n",
    "    cnt = zh_txt.count(v)\n",
    "    zh_vocab_dict[key] = cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b7e04",
   "metadata": {},
   "source": [
    "length of vocab count dictionary for Korean and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9356c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336782\n",
      "183119\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_vocab_dict))\n",
    "print(len(zh_vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d17a8",
   "metadata": {},
   "source": [
    "# process BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3184551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Tuple, List, Set\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd59ae",
   "metadata": {},
   "source": [
    "define a function to count the frequency of each vocab pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e33cc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"Get counts of pairs of consecutive symbols.\"\"\"\n",
    "\n",
    "    pairs = {}\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        # count occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            current_frequency = pairs.get(pair, 0)\n",
    "            pairs[pair] = current_frequency + frequency\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b85fd",
   "metadata": {},
   "source": [
    "define a function to merge the most frequent vocab pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7709344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(best_pair: Tuple[str, str], vocab_in: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "\n",
    "    vocab_out = {}\n",
    "\n",
    "    # re.escape\n",
    "    # ensures the characters of our input pair will be handled as is and\n",
    "    # not get mistreated as special characters in the regular expression.\n",
    "    pattern = re.escape(' '.join(best_pair))\n",
    "    replacement = ''.join(best_pair)\n",
    "\n",
    "    for word_in in vocab_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        word_out = re.sub(pattern, replacement, word_in)\n",
    "        vocab_out[word_out] = vocab_in[word_in]\n",
    "\n",
    "    return vocab_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96aaa5",
   "metadata": {},
   "source": [
    "we update the Korean vocab list with most frequent pairs by iterating the loop 32000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cca9336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing BPE for korean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 32000/32000 [5:20:31<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# we store the best pair during each iteration for encoding new vocabulary, more on this later\n",
    "\n",
    "ko_bpe_codes = {}\n",
    "num_merges = 32000  # hyperparameter\n",
    "\n",
    "print('processing BPE for korean...')\n",
    "for i in tqdm(range(num_merges)):\n",
    "    pair_stats = get_pair_stats(ko_vocab_dict)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    ko_bpe_codes[best_pair] = i\n",
    "    \n",
    "    ko_vocab_dict = merge_vocab(best_pair, ko_vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bef97f",
   "metadata": {},
   "source": [
    "In the same way, we update the Chinese vocab list with most frequent pairs by iterating the loop 32000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc174d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing BPE for chinese...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 32000/32000 [18:00:17<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "zh_bpe_codes = {}\n",
    "num_merges = 32000  # hyperparameter\n",
    "\n",
    "print('processing BPE for chinese...')\n",
    "for i in tqdm(range(num_merges)):\n",
    "    pair_stats = get_pair_stats(zh_vocab_dict)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    zh_bpe_codes[best_pair] = i\n",
    "    \n",
    "    zh_vocab_dict = merge_vocab(best_pair, zh_vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea119d",
   "metadata": {},
   "source": [
    "the number of new BPE token made as result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b9c3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_bpe_codes))\n",
    "print(len(zh_bpe_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf051749",
   "metadata": {},
   "source": [
    "# functions for applying BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264669b",
   "metadata": {},
   "source": [
    "define a funtion to get BPE pairs from input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e761123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word: List[str]) -> Set[Tuple[str, str]]:\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053714e",
   "metadata": {},
   "source": [
    "define a function to create new words with BPE pairs from input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "348c6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_word(word: List[str], pair_to_merge: Tuple[str, str]) -> List[str]:\n",
    "    first, second = pair_to_merge\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        try:\n",
    "            j = word.index(first, i)\n",
    "            new_word.extend(word[i:j])\n",
    "            i = j\n",
    "        except ValueError:\n",
    "            new_word.extend(word[i:])\n",
    "            break\n",
    "\n",
    "        if i < len(word) - 1 and word[i + 1] == second:\n",
    "            new_word.append(first + second)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word.append(first)\n",
    "            i += 1\n",
    "\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff763e0f",
   "metadata": {},
   "source": [
    "define a function that performs the whole BPE process with the pre-defined BPE functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "970f7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(original_word: str, bpe_codes: Dict[Tuple[str, str], int]) -> List[str]:\n",
    "    if len(original_word) == 1:\n",
    "        return original_word\n",
    "\n",
    "    word = list(original_word)\n",
    "    word.append('</w>')\n",
    "\n",
    "    while True:\n",
    "        pairs = get_pairs(word)\n",
    "        bpe_codes_pairs = [(pair, bpe_codes[pair]) for pair in pairs if pair in bpe_codes]\n",
    "        if not bpe_codes_pairs:\n",
    "            break\n",
    "\n",
    "        pair_to_merge = min(bpe_codes_pairs, key=itemgetter(1))[0]\n",
    "        word = create_new_word(word, pair_to_merge)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535a3bf",
   "metadata": {},
   "source": [
    "# apply BPE to sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e7462",
   "metadata": {},
   "source": [
    "apply BPE to Korean sentences and append to ko_txt variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af54ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_txt = ''\n",
    "for i, sent in enumerate(ko):\n",
    "    encoded = encode(sent, ko_bpe_codes)\n",
    "    \n",
    "    txt = ''\n",
    "    for e in encoded:\n",
    "        txt += str(e) + ' '\n",
    "    txt = txt[:-1] + '\\n'\n",
    "    ko_txt += txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040736b",
   "metadata": {},
   "source": [
    "apply BPE to Chinese sentences and append to zh_txt variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f253c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_txt = ''\n",
    "for i, sent in enumerate(zh):\n",
    "    encoded = encode(sent, zh_bpe_codes)\n",
    "    \n",
    "    txt = ''\n",
    "    for e in encoded:\n",
    "        txt += str(e) + ' '\n",
    "    txt = txt[:-1] + '\\n'\n",
    "    zh_txt += txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16293f",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4bc79",
   "metadata": {},
   "source": [
    "save each BPE text as text file to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebb482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/labdata/ko_bpe.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(ko_txt)\n",
    "    \n",
    "with open('../data/labdata/zh_bpe.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(zh_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dd4ad",
   "metadata": {},
   "source": [
    "# reprocessing\n",
    "- remove \\</w> token at the end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f078f4",
   "metadata": {},
   "source": [
    "read files from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d259de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/labdata/ko_bpe.txt', 'r', encoding='utf-8') as f:\n",
    "    ko = f.read()\n",
    "    \n",
    "with open('../data/labdata/zh_bpe.txt', 'r', encoding='utf-8') as f:\n",
    "    zh = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ad124",
   "metadata": {},
   "source": [
    "split each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed8042ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_ko = ko.split('\\n')\n",
    "ls_zh = zh.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7e1cb",
   "metadata": {},
   "source": [
    "exclude the last empty element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c68e89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_ko = ls_ko[:-1]\n",
    "ls_zh = ls_zh[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7ea8b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "180000\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_ko))\n",
    "print(len(ls_ko))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310cb06",
   "metadata": {},
   "source": [
    "remove \\</w> token for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df9c9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ko = []\n",
    "r_zh = []\n",
    "for i in range(len(ls_ko)):\n",
    "    ko_tmp = ls_ko[i].replace('</w>' ,'').strip()\n",
    "    zh_tmp = ls_zh[i].replace('</w>' ,'').strip()\n",
    "    \n",
    "    r_ko.append(ko_tmp)\n",
    "    r_zh.append(zh_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a76675",
   "metadata": {},
   "source": [
    "example of last 10 Chinese sentences split into BPE tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d4c4859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在 晓星 的 越南 氨 纶 工厂 当地 工人 正在 确认 产品的 质量',\n",
       " '另外 此前 明 亮 眼 科 21 医院 向 绿色雨伞儿童财团 支援了 为 新冠疫情 弱势群体儿童 一代 的50 箱 绿色 食品 套装',\n",
       " '从本月开始 金融 广播 研究开发 等 例外 领域 也要 义务 实行每周52小时制 因此 财界 担心 这会 降低 出口 竞争 力',\n",
       " '据悉 劳资双方 直到 达成协议 为止一直 在进行 密切 的 私下 对话',\n",
       " '今年年初 副总理兼 企划财政 部长 官 洪南基 虽然也 说过 雇佣 成功 实现了 V 型 反弹 等 脱离 现实 的话 但是 执政党 内 没有 人 指出 这一点',\n",
       " '另外雇佣部 计划向 外国人 多数 雇佣 工厂 等对 新型 日冕 脆弱的 建设 制造 服务业 事业场 紧急支援 7 2万 个 口罩',\n",
       " '据全国 公务员 工会 釜山地区 总部 透露 28 日 上午8点 左右在 釜山市 政府 大厅 釜山市 公务员 工会和 民主总工会 釜山本部 的50 多名 成员 拿着 牌子 展开了 宣传 战',\n",
       " '道 计划 将以 此次 投资协议 为契机 继续努力 吸引 青年 及 女性雇佣 比率 较高的 知识 信息 文化 领域的 投资 为 搞活地区经济 和 创出工作岗位 做出贡献',\n",
       " '幸运 的是 随着政府 部门和 首尔市 等 地方自治 团 在校 期间 提供了 最低工资 水平 的工资 在 大幅增加 体验 实习 的 项目',\n",
       " '也就是说即使 顺利 出售 也会 被迫 所谓 结构调整 的不 可避免 的选择']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_zh[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905c921",
   "metadata": {},
   "source": [
    "list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38d31e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_ko = np.array(r_ko)\n",
    "np_zh = np.array(r_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9145",
   "metadata": {},
   "source": [
    "save them as text to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "39aa5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/labdata/ko_bpe_180k', np_ko, delimiter='\\n', fmt='%s')\n",
    "np.savetxt('../data/labdata/zh_bpe_180k', np_zh, delimiter='\\n', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7617d",
   "metadata": {},
   "source": [
    "we will take only 100000 sentences for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eefbaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_100k = np_ko[:100000]\n",
    "zh_100k = np_zh[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3bf99b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_100k))\n",
    "print(len(zh_100k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493c4ff",
   "metadata": {},
   "source": [
    "# save as train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2241873",
   "metadata": {},
   "source": [
    "split each corpus to 8:2 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6dc09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ko_train, ko_test, zh_train, zh_test = train_test_split(ko_100k, zh_100k, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bb33d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "80000\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_train))\n",
    "print(len(zh_train))\n",
    "print(len(ko_test))\n",
    "print(len(zh_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481f4d7",
   "metadata": {},
   "source": [
    "save them to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "95fd499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/labdata/ko_train.txt', ko_train, delimiter='\\n', fmt='%s')\n",
    "np.savetxt('../data/labdata/ko_test.txt', ko_test, delimiter='\\n', fmt='%s')\n",
    "np.savetxt('../data/labdata/zh_train.txt', zh_train, delimiter='\\n', fmt='%s')\n",
    "np.savetxt('../data/labdata/zh_test.txt', zh_test, delimiter='\\n', fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
